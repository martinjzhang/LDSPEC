{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ldspec\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescale tau_star by SD and a factor such that h2 are positive\n",
      "    v_h2_ld    min=-0.114, max=0.153\n",
      "    scale_factor_legit=6.997\n",
      "    scale_factor_enrich=19.680\n",
      "    scale_factor=6.997\n",
      "LD architecture\n",
      "    v_h2_ld    min=-0.800, max=1.070\n",
      "    AN:alleleage_common                      -0.094, enrich_q20=1.148, enrich_q80=0.808\n",
      "    AN:LLD_AFR_common                        -0.085, enrich_q20=1.088, enrich_q80=0.952\n",
      "    AN:LLD_AFR_lf                            -0.094, enrich_q20=1.070, enrich_q80=0.905\n",
      "    AN:recomb_rate_common                    -0.022, enrich_q20=1.094, enrich_q80=0.867\n",
      "    AN:recomb_rate_lf                        -0.023, enrich_q20=1.126, enrich_q80=0.841\n",
      "    AN:nucleotide_div_common                 -0.024, enrich_q20=1.130, enrich_q80=0.853\n",
      "    AN:nucleotide_div_lf                     -0.024, enrich_q20=1.125, enrich_q80=0.857\n",
      "    AN:Backgrd_Selection_Stat_common         0.172, enrich_q20=0.916, enrich_q80=1.123\n",
      "    AN:Backgrd_Selection_Stat_lf             0.158, enrich_q20=0.891, enrich_q80=1.147\n",
      "    AN:CpG_common                            2.277, enrich_q20=0.956, enrich_q80=1.118\n",
      "    AN:CpG_lf                                2.100, enrich_q20=0.944, enrich_q80=1.094\n",
      "AN_list 165\n",
      "pAN_list 136\n"
     ]
    }
   ],
   "source": [
    "# dic_config_ld\n",
    "df_annot = ldspec.util.read_annot(\n",
    "    '/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001/baseline_annot/baseline_165annots_chr22.annot.gz'\n",
    ")\n",
    "\n",
    "dic_tau_star = {\n",
    "    'AN:alleleage_common' : -0.24, \n",
    "    'AN:LLD_AFR_common' : -0.20, \n",
    "    'AN:LLD_AFR_lf' : -0.20,     \n",
    "    'AN:recomb_rate_common' : -0.20, \n",
    "    'AN:recomb_rate_lf' : -0.20, \n",
    "    'AN:nucleotide_div_common' : -0.13, \n",
    "    'AN:nucleotide_div_lf' : -0.13,     \n",
    "    'AN:Backgrd_Selection_Stat_common' : 0.11, \n",
    "    'AN:Backgrd_Selection_Stat_lf' : 0.11,     \n",
    "    'AN:CpG_common' : 0.23, \n",
    "    'AN:CpG_lf' : 0.23,  \n",
    "}\n",
    "\n",
    "print('Rescale tau_star by SD and a factor such that h2 are positive')\n",
    "ind_common = df_annot[['AN:mbin%d_common'%x for x in range(10)]].sum(axis=1).values>0\n",
    "ind_lf = df_annot[['AN:mbin%d_lf'%x for x in range(5)]].sum(axis=1).values>0\n",
    "for AN in dic_tau_star:\n",
    "    ind_select = ind_common if AN.endswith('_common') else ind_lf\n",
    "    dic_tau_star[AN] = dic_tau_star[AN] / df_annot.loc[ind_select, AN].std() / 20\n",
    "v_h2_ld = df_annot[dic_tau_star].dot([dic_tau_star[x] for x in dic_tau_star])\n",
    "print('    v_h2_ld    min=%0.3f, max=%0.3f' % (v_h2_ld.min(), v_h2_ld.max()))\n",
    "\n",
    "enrich_max = 1.5 # maximum enrich of 1.5\n",
    "scale_list = []\n",
    "for AN in dic_tau_star:\n",
    "    ind_select = ind_common if AN.endswith('_common') else ind_lf\n",
    "    q20 = np.quantile(df_annot.loc[ind_select, AN], 0.2)\n",
    "    q80 = np.quantile(df_annot.loc[ind_select, AN], 0.8)\n",
    "    avg_all = v_h2_ld[ind_select].mean()\n",
    "    avg_q80 = v_h2_ld[ind_select & (df_annot[AN].values>=q80)].mean() \n",
    "    avg_q20 = v_h2_ld[ind_select & (df_annot[AN].values<=q20)].mean()\n",
    "    avg_qmax = max(avg_q20, avg_q80)\n",
    "    scale_list.append( (enrich_max - 1) / (avg_qmax - avg_all * enrich_max))\n",
    "scale_factor_legit = -0.8 / v_h2_ld.min()\n",
    "print('    scale_factor_legit=%0.3f' % scale_factor_legit)\n",
    "scale_factor_enrich = np.min(scale_list)\n",
    "print('    scale_factor_enrich=%0.3f' % scale_factor_enrich)\n",
    "scale_factor = min(scale_factor_enrich, scale_factor_legit)\n",
    "print('    scale_factor=%0.3f' % scale_factor)\n",
    "\n",
    "dic_config_ld = {}\n",
    "print('LD architecture')\n",
    "for AN in dic_tau_star:\n",
    "    dic_config_ld[AN] = dic_tau_star[AN] * scale_factor\n",
    "    \n",
    "v_h2_ld = df_annot[dic_config_ld].dot([dic_config_ld[x] for x in dic_config_ld])\n",
    "print('    v_h2_ld    min=%0.3f, max=%0.3f' % (v_h2_ld.min(), v_h2_ld.max()))\n",
    "for AN in dic_tau_star:\n",
    "    ind_select = ind_common if AN.endswith('_common') else ind_lf\n",
    "    q20 = np.quantile(df_annot.loc[ind_select, AN], 0.2)\n",
    "    q80 = np.quantile(df_annot.loc[ind_select, AN], 0.8)\n",
    "    avg_all = v_h2_ld[ind_select].mean()\n",
    "    avg_q80 = v_h2_ld[ind_select & (df_annot[AN].values>=q80)].mean() \n",
    "    avg_q20 = v_h2_ld[ind_select & (df_annot[AN].values<=q20)].mean()\n",
    "    print('    %-40s %0.3f, enrich_q20=%0.3f, enrich_q80=%0.3f' % (\n",
    "        AN, dic_config_ld[AN], (avg_q20+1) / (avg_all+1), (avg_q80+1) / (avg_all+1),\n",
    "    ))\n",
    "    \n",
    "dic_config_mbin = {}\n",
    "dic_config_mbin.update({'AN:mbin%d_common'%x : 0 for x in range(10)})\n",
    "dic_config_mbin.update({'AN:mbin%d_lf'%x : 0 for x in range(5)})\n",
    "\n",
    "# AN_list, pAN_list\n",
    "reg_annot_list = [\n",
    "    '/home/jz286/WES_analysis/LDSPEC/experiments/job.analysis_imp_geno_chimp/reg_annot_file/'\n",
    "    'reg_annot_file.prox_gene_fct_all_ld.txt',\n",
    "]\n",
    "\n",
    "annot_file_list,AN_list,pAN_list = [],[],[]\n",
    "for reg_annot_file in reg_annot_list:\n",
    "    annot_file_list += [x for x in pd.read_csv(reg_annot_file, header=None)[0] if x not in annot_file_list]\n",
    "for annot_file in annot_file_list:\n",
    "    annot_name = ldspec.util.get_annot_name_from_file(annot_file)\n",
    "    if annot_file.endswith(\".annot.gz\"):\n",
    "        temp_df = ldspec.util.read_annot(annot_file.replace('@', '1'), nrows=5)\n",
    "        AN_list.extend([x for x in temp_df if x.startswith(\"AN:\")])\n",
    "    if annot_file.endswith(\".pannot_mat.npz\"):\n",
    "        pAN_list.append(annot_name)\n",
    "print('AN_list', len(AN_list))\n",
    "print('pAN_list', len(pAN_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null_h2g50_p20\n",
      "causal_h2g50_p20\n",
      "causal_neg_h2g50_p20\n",
      "causal_pos_h2g50_p20\n",
      "null_h2g50_p10\n",
      "causal_h2g50_p10\n",
      "null_h2g20_p20\n",
      "causal_h2g20_p20\n"
     ]
    }
   ],
   "source": [
    "dic_config = {}\n",
    "alpha = -0.38\n",
    "OUT_PATH = '/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123'\n",
    "\n",
    "for h2g,p_causal in [[0.5, 0.2], [0.5, 0.1], [0.2, 0.2]]:\n",
    "    dic_config['null_h2g%d_p%d' % (h2g*100, p_causal*100)] = {\n",
    "        'basic' : (h2g, p_causal, alpha, True), # h2g, sparsity, alpha, LD-dependent\n",
    "        'AN:all' : 1,\n",
    "        'AN:SuperEnhancer_Hnisz_common' : 2,\n",
    "    }    \n",
    "        \n",
    "    dic_config['causal_h2g%d_p%d' % (h2g*100, p_causal*100)] = {\n",
    "        'basic' : (h2g, p_causal, alpha, True), # h2g, sparsity, alpha, LD-dependent\n",
    "        'AN:all' : 1,\n",
    "        'AN:SuperEnhancer_Hnisz_common' : 2,\n",
    "        # pos_ld\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_common_block' : -0.5,\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_lf_block' : -0.5, \n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_common_block': -0.3,\n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_lf_block' : -0.3, \n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_common_block' : -0.3,\n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_lf_block' : -0.3,\n",
    "    }\n",
    "    \n",
    "    if [h2g,p_causal] != [0.5, 0.2]:\n",
    "        continue\n",
    "        \n",
    "    dic_config['causal_neg_h2g%d_p%d' % (h2g*100, p_causal*100)] = {\n",
    "        'basic' : (h2g, p_causal, alpha, True), # h2g, sparsity, alpha, LD-dependent\n",
    "        'AN:all' : 1,\n",
    "        'AN:SuperEnhancer_Hnisz_common' : 2,\n",
    "        # pos_ld\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_common_block' : -0.5,\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_lf_block' : -0.5, \n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_common_block': -0.3,\n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_lf_block' : -0.3, \n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_common_block' : -0.3,\n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_lf_block' : -0.3,\n",
    "         # neg_ld\n",
    "        'pAN:proxy_0_100_ld_n100_p0_maf_common_block' : -0.5,\n",
    "        'pAN:proxy_0_100_ld_n100_p0_maf_lf_block' : -0.5,  \n",
    "        'pAN:proxy_100_1000_ld_n100_p0_maf_common_block': -0.3,\n",
    "        'pAN:proxy_100_1000_ld_n100_p0_maf_lf_block' : -0.3, \n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_n100_p0_maf_common_block' : -0.3,\n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_n100_p0_maf_lf_block' : -0.3,\n",
    "    }\n",
    "    \n",
    "    dic_config['causal_pos_h2g%d_p%d' % (h2g*100, p_causal*100)] = {\n",
    "        'basic' : (h2g, p_causal, alpha, True), # h2g, sparsity, alpha, LD-dependent\n",
    "        'AN:all' : 1,\n",
    "        'AN:SuperEnhancer_Hnisz_common' : 2,\n",
    "        # pos_ld\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_common_block' : 0.5,\n",
    "        'pAN:proxy_0_100_ld_p0_p100_maf_lf_block' : 0.5, \n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_common_block': 0.3,\n",
    "        'pAN:proxy_100_1000_ld_p0_p100_maf_lf_block' : 0.3, \n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_common_block' : 0.3,\n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_p0_p100_maf_lf_block' : 0.3,\n",
    "         # neg_ld\n",
    "        'pAN:proxy_0_100_ld_n100_p0_maf_common_block' : 0.5,\n",
    "        'pAN:proxy_0_100_ld_n100_p0_maf_lf_block' : 0.5,  \n",
    "        'pAN:proxy_100_1000_ld_n100_p0_maf_common_block': 0.3,\n",
    "        'pAN:proxy_100_1000_ld_n100_p0_maf_lf_block' : 0.3, \n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_n100_p0_maf_common_block' : 0.3,\n",
    "        'pAN:SuperEnhancer_Hnisz_proxy_0_1000_ld_n100_p0_maf_lf_block' : 0.3,\n",
    "    }\n",
    "\n",
    "fpath_list = []\n",
    "for simu in dic_config:\n",
    "    print(simu)\n",
    "    if set(dic_config[simu]) - set(AN_list + pAN_list) != set(['basic']):\n",
    "        print('    Missing: ' + ','.join([x for x in dic_config[simu] if x not in AN_list + pAN_list]))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(OUT_PATH+'/%s' % simu)\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    fpath_list.append(OUT_PATH+'/%s' % simu)\n",
    "    \n",
    "    with open(OUT_PATH+'/%s/config' % simu, 'w') as f:\n",
    "        h2g,p_causal,alpha,flag_ld = dic_config[simu]['basic']\n",
    "        f.write('h2g\\t%0.3f\\n' % h2g)\n",
    "        f.write('p_causal\\t%0.3f\\n' % p_causal)\n",
    "        f.write('alpha\\t%0.3f\\n' % alpha)\n",
    "        if flag_ld:\n",
    "            for AN in dic_config_ld:\n",
    "                f.write('%s\\t%0.3f\\n' % (AN, dic_config_ld[AN]))\n",
    "        for AN in dic_config_mbin:\n",
    "            f.write('%s\\t%0.3f\\n' % (AN, dic_config_mbin[AN]))\n",
    "        for AN in [x for x in dic_config[simu] if x!='basic']:\n",
    "            f.write('%s\\t%0.3f\\n' % (AN, dic_config[simu][AN]))\n",
    "            \n",
    "with open(OUT_PATH+'/simu_list.txt', 'w') as f:\n",
    "    for fpath in fpath_list:\n",
    "        f.write('%s\\n' % fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/causal_h2g20_p20\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/causal_h2g50_p10\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/causal_h2g50_p20\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/causal_neg_h2g50_p20\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/causal_pos_h2g50_p20\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/null_h2g20_p20\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/null_h2g50_p10\n",
      "/n/groups/price/martin/LDSPEC_data/UKBimp_337K_MAF001_chimp/simulation.100123/null_h2g50_p20\n"
     ]
    }
   ],
   "source": [
    "# Convert config files into a table\n",
    "df_list = []\n",
    "simu_list_paper = [\n",
    "    'null_h2g50_p20', 'null_h2g50_p10', 'null_h2g20_p20',\n",
    "    'causal_h2g50_p20', 'causal_h2g50_p10', 'causal_h2g20_p20',\n",
    "    'causal_neg_h2g50_p20', 'causal_pos_h2g50_p20',\n",
    "]\n",
    "for simu in sorted(pd.read_csv(OUT_PATH+'/simu_list.txt', header=None)[0]):\n",
    "    simu_sf = simu.split('/')[-1]\n",
    "    if simu_sf not in simu_list_paper:\n",
    "        continue\n",
    "    print(simu)\n",
    "    temp_df = pd.read_csv(simu+'/config', header=None, delim_whitespace=True)\n",
    "    temp_df = temp_df.loc[temp_df[1]!=0]\n",
    "    temp_df[2] = simu.split('/')[-1]\n",
    "    temp_df = temp_df[[2,0,1]]\n",
    "    temp_df.columns = ['simulation', 'term', 'value']\n",
    "    df_list.append(temp_df)\n",
    "df_config = pd.concat(df_list, axis=0)\n",
    "df_config.to_csv('/n/groups/price/martin/LDSPEC_data/results/tables/simu_param.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_phen = pd.read_csv(\n",
    "#     '/n/groups/price/martin/data_GDREG/UKBimp_337K_MAF001_chimp/simulation.040123/null_h2g50_p20/rep0.phen',\n",
    "#     sep='\\t'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
